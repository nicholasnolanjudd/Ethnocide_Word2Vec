{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8VlsVSozokB"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json, time, random\n",
        "\n",
        "\n",
        "def get_html_content(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            return response.text\n",
        "        else:\n",
        "            print(f\"Error: Unable to fetch HTML. Status code: {response.status_code}\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def get_items(content):\n",
        "    try:\n",
        "        soup = BeautifulSoup(content, 'html.parser')\n",
        "        links = soup.find_all(class_='link db fb fs18')\n",
        "        return [link.attrs[\"href\"] for link in links]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during text extraction: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    url = \"https://www.mfa.am/en/press-releases/\"\n",
        "\n",
        "    data = {}\n",
        "\n",
        "    for i in range(425):\n",
        "\n",
        "        latency = random.randint(5, 20)\n",
        "        # print(latency)\n",
        "        print(i)\n",
        "        # time.sleep(latency)\n",
        "        html_content = get_html_content(url + \"?page=\" + str(i))\n",
        "        items = get_items(html_content)\n",
        "\n",
        "        for j, item in enumerate(items):\n",
        "            try:\n",
        "                html = get_html_content(item)\n",
        "                soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "                textDiv = soup.find('div',attrs={\"class\":\"static-content\"})\n",
        "                data[str(i) + \",\" + str(j)] = [text.text for text in textDiv.find_all('p')]\n",
        "\n",
        "            except:\n",
        "                data[str(i) + \",\" + str(j)] = []\n",
        "\n",
        "\n",
        "    with open(\"data.json\", \"w\") as f:\n",
        "        json.dump(data, f)\n"
      ]
    }
  ]
}